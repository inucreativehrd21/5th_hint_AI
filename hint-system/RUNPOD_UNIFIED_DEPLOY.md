# ğŸš€ RunPod vLLM í†µí•© ì´ë¯¸ì§€ ë°°í¬ ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

vLLM ê³µì‹ ì´ë¯¸ì§€ë¥¼ ë² ì´ìŠ¤ë¡œ Gradio UIê°€ í†µí•©ëœ **ë‹¨ì¼ Docker ì´ë¯¸ì§€**ë¥¼ ìƒì„±í•˜ì—¬ RunPodì—ì„œ "ë”¸ê¹" ë°°í¬í•©ë‹ˆë‹¤.

---

## ğŸ“¦ ì¤€ë¹„ë¬¼

- Docker Desktop ì„¤ì¹˜ (ë¡œì»¬ ë¹Œë“œìš©)
- Docker Hub ê³„ì •
- RunPod ê³„ì •
- GPU: RTX 4090/5090 (24GB VRAM ê¶Œì¥)

---

## ğŸ”¨ 1ë‹¨ê³„: Docker ì´ë¯¸ì§€ ë¹Œë“œ

### 1-1. í”„ë¡œì íŠ¸ í´ë¡  (ë¡œì»¬)

```bash
git clone https://github.com/inucreativehrd21/5th_hint_AI.git
cd 5th_hint_AI/hint-system
```

### 1-2. Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
# Docker Hub ì‚¬ìš©ìëª… ì„¤ì •
DOCKER_USERNAME="your-dockerhub-username"

# ì´ë¯¸ì§€ ë¹Œë“œ (5-10ë¶„ ì†Œìš”)
docker build -f Dockerfile.unified -t ${DOCKER_USERNAME}/hint-ai-vllm:latest .
```

**ë¹Œë“œ ë‚´ìš©:**
- âœ… vLLM ê³µì‹ ì´ë¯¸ì§€ ë² ì´ìŠ¤ (`vllm/vllm-openai:latest`)
- âœ… Gradio 4.44.0 + í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
- âœ… ì•± ì½”ë“œ ë³µì‚¬ ë° í™˜ê²½ ì„¤ì •
- âœ… vLLM + Gradio ë™ì‹œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

### 1-3. ë¡œì»¬ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)

```bash
# GPU ìˆëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸
docker run --gpus all \
  -p 8000:8000 \
  -p 7860:7860 \
  --ipc=host \
  --shm-size=16gb \
  ${DOCKER_USERNAME}/hint-ai-vllm:latest

# ì ‘ì† í…ŒìŠ¤íŠ¸
curl http://localhost:8000/health
# ë¸Œë¼ìš°ì €: http://localhost:7860
```

---

## ğŸ“¤ 2ë‹¨ê³„: Docker Hubì— í‘¸ì‹œ

### 2-1. Docker Hub ë¡œê·¸ì¸

```bash
docker login
# Username: your-dockerhub-username
# Password: your-dockerhub-password
```

### 2-2. ì´ë¯¸ì§€ í‘¸ì‹œ

```bash
docker push ${DOCKER_USERNAME}/hint-ai-vllm:latest
```

**í‘¸ì‹œ ì™„ë£Œ í›„ ì´ë¯¸ì§€ ì£¼ì†Œ:**
```
your-dockerhub-username/hint-ai-vllm:latest
```

---

## ğŸ® 3ë‹¨ê³„: RunPodì—ì„œ Pod ìƒì„±

### 3-1. RunPod ì›¹ì‚¬ì´íŠ¸ ì ‘ì†

https://www.runpod.io â†’ ë¡œê·¸ì¸

### 3-2. Pod ìƒì„±

1. **"Deploy"** í´ë¦­
2. **GPU ì„ íƒ**:
   - **Secure Cloud** ë˜ëŠ” **Community Cloud**
   - **GPU**: RTX 4090 ë˜ëŠ” RTX 5090
   - **VRAM**: 24GB ê¶Œì¥

3. **í…œí”Œë¦¿ ì„¤ì •**:
   - **"Use Custom Image"** í´ë¦­
   - **Container Image**: `your-dockerhub-username/hint-ai-vllm:latest`
   
4. **ì¶”ê°€ ì„¤ì •**:
   - **Container Disk**: 50GB ì´ìƒ
   - **Volume Disk**: ì„ íƒì‚¬í•­
   - **Expose Ports**: `8000, 7860`
   
5. **í™˜ê²½ ë³€ìˆ˜** (ì„ íƒì‚¬í•­):
   ```
   HF_TOKEN=your_huggingface_token_here
   VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct
   ```

6. **"Deploy"** í´ë¦­!

---

## âœ… 4ë‹¨ê³„: Pod êµ¬ë™ í™•ì¸

### 4-1. Pod ì‹œì‘ ëŒ€ê¸°

- Pod ìƒì„± í›„ **5-10ë¶„** ëŒ€ê¸°
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + vLLM ì„œë²„ ì‹œì‘ ì‹œê°„

### 4-2. ë¡œê·¸ í™•ì¸

RunPod ëŒ€ì‹œë³´ë“œì—ì„œ:
1. Pod í´ë¦­ â†’ **"Logs"** íƒ­
2. ë‹¤ìŒ ë©”ì‹œì§€ í™•ì¸:
   ```
   ğŸš€ Starting vLLM + Gradio Hint System...
   ğŸ“¦ Starting vLLM server on port 8000...
   âœ… vLLM server is ready!
   ğŸ¨ Starting Gradio UI on port 7860...
   ```

### 4-3. ì ‘ì†

#### ë°©ë²• 1: RunPod í¬íŠ¸ ë§¤í•‘ (ì¶”ì²œ)

1. Pod ìƒì„¸ í˜ì´ì§€ â†’ **"Connect"** íƒ­
2. **"TCP Port Mappings"** ì„¹ì…˜ì—ì„œ ì™¸ë¶€ URL í™•ì¸:
   ```
   7860 â†’ https://xxxxx-7860.proxy.runpod.net (Gradio UI)
   8000 â†’ https://xxxxx-8000.proxy.runpod.net (vLLM API)
   ```

#### ë°©ë²• 2: SSH í„°ë„ë§

```bash
# RunPod SSH ì ‘ì† ì •ë³´ í™•ì¸
ssh -p <SSH_PORT> root@<POD_IP>

# í„°ë„ë§ (ë¡œì»¬ ë¨¸ì‹ ì—ì„œ)
ssh -L 7860:localhost:7860 -L 8000:localhost:8000 -p <SSH_PORT> root@<POD_IP>

# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
http://localhost:7860  # Gradio UI
http://localhost:8000  # vLLM API
```

---

## ğŸ§ª 5ë‹¨ê³„: ì‘ë™ í…ŒìŠ¤íŠ¸

### 5-1. vLLM API í…ŒìŠ¤íŠ¸

```bash
# Health Check
curl https://xxxxx-8000.proxy.runpod.net/health

# ëª¨ë¸ ì •ë³´
curl https://xxxxx-8000.proxy.runpod.net/v1/models
```

### 5-2. Gradio UI í…ŒìŠ¤íŠ¸

1. Gradio URL ì ‘ì†: `https://xxxxx-7860.proxy.runpod.net`
2. ë¬¸ì œ ë²ˆí˜¸ ì…ë ¥ (ì˜ˆ: `1000`)
3. ì½”ë“œ ì‘ì„±
4. **"ğŸ’¡ íŒíŠ¸ ë°›ê¸°"** í´ë¦­
5. íŒíŠ¸ ìƒì„± í™•ì¸ (3-5ì´ˆ ì†Œìš”)

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

### Pod ë‚´ë¶€ì—ì„œ í™•ì¸

```bash
# SSH ì ‘ì†
ssh -p <SSH_PORT> root@<POD_IP>

# GPU ì‚¬ìš©ëŸ‰
nvidia-smi

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep python

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
free -h

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
df -h
```

### RunPod ëŒ€ì‹œë³´ë“œ

- **"Stats"** íƒ­ì—ì„œ ì‹¤ì‹œê°„ GPU/CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: Podì´ ì‹œì‘ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: Podì´ "Pending" ìƒíƒœì—ì„œ ë©ˆì¶¤

**í•´ê²°:**
```bash
# ë¡œê·¸ í™•ì¸
RunPod Dashboard â†’ Logs

# ì¼ë°˜ì ì¸ ì›ì¸:
1. ì´ë¯¸ì§€ pull ì‹¤íŒ¨ â†’ Docker Hub ì´ë¯¸ì§€ ì£¼ì†Œ í™•ì¸
2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ ë” í° GPU ì„ íƒ
3. ë””ìŠ¤í¬ ë¶€ì¡± â†’ Container Disk í¬ê¸° ì¦ê°€ (50GB+)
```

### ë¬¸ì œ 2: vLLM ì„œë²„ ì‹œì‘ ì‹¤íŒ¨

**ì¦ìƒ**: ë¡œê·¸ì— "CUDA out of memory" ì—ëŸ¬

**í•´ê²°:**
```bash
# í™˜ê²½ë³€ìˆ˜ ì¶”ê°€ (GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê°ì†Œ)
VLLM_GPU_MEMORY_UTILIZATION=0.75

# ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-1.5B-Instruct
```

### ë¬¸ì œ 3: Gradio UI ì ‘ì† ì•ˆ ë¨

**ì¦ìƒ**: vLLMì€ ì •ìƒì´ì§€ë§Œ Gradioê°€ ì•ˆ ì—´ë¦¼

**í•´ê²°:**
```bash
# SSH ì ‘ì† í›„ ìˆ˜ë™ í™•ì¸
curl http://localhost:7860

# ë¡œê·¸ í™•ì¸
docker logs <container_id>

# í¬íŠ¸ ë…¸ì¶œ í™•ì¸
RunPod Dashboard â†’ Connect â†’ TCP Port Mappings
```

### ë¬¸ì œ 4: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼

**ì¦ìƒ**: HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ë§¤ìš° ëŠë¦¼

**í•´ê²°:**
```bash
# HuggingFace í† í° ì„¤ì • (í™˜ê²½ë³€ìˆ˜)
HF_TOKEN=your_token_here

# ë˜ëŠ” ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš© (Volume ë§ˆìš´íŠ¸)
# RunPod Volumeì— ëª¨ë¸ ì €ì¥ í›„ ë§ˆìš´íŠ¸
```

---

## ğŸ“ ìœ ìš©í•œ ëª…ë ¹ì–´

### Docker Hub ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸

```bash
# ì½”ë“œ ìˆ˜ì • í›„ ì¬ë¹Œë“œ
docker build -f Dockerfile.unified -t ${DOCKER_USERNAME}/hint-ai-vllm:latest .

# ìƒˆ ë²„ì „ íƒœê·¸ ì¶”ê°€
docker tag ${DOCKER_USERNAME}/hint-ai-vllm:latest ${DOCKER_USERNAME}/hint-ai-vllm:v1.0

# í‘¸ì‹œ
docker push ${DOCKER_USERNAME}/hint-ai-vllm:latest
docker push ${DOCKER_USERNAME}/hint-ai-vllm:v1.0
```

### RunPod Pod ì¬ì‹œì‘

```bash
# Pod ì¤‘ì§€
RunPod Dashboard â†’ Stop Pod

# Pod ì‹œì‘
RunPod Dashboard â†’ Start Pod

# ë˜ëŠ” SSH ì ‘ì† í›„ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart <container_id>
```

---

## ğŸ¯ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

ë°°í¬ ì™„ë£Œ í™•ì¸:

- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ
- [ ] Docker Hub í‘¸ì‹œ ì™„ë£Œ
- [ ] RunPod Pod ìƒì„± ì™„ë£Œ
- [ ] vLLM ì„œë²„ ì •ìƒ ì‘ë™ (`/health` ì‘ë‹µ í™•ì¸)
- [ ] Gradio UI ì ‘ì† ê°€ëŠ¥
- [ ] íŒíŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] ì™¸ë¶€ URLë¡œ ì ‘ì† ê°€ëŠ¥
- [ ] ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì •ìƒ (GPU ë©”ëª¨ë¦¬ 80% ì´í•˜)

---

## ğŸš€ ìš”ì•½

### ë¡œì»¬ì—ì„œ (í•œ ë²ˆë§Œ)

```bash
# 1. ë¹Œë“œ
docker build -f Dockerfile.unified -t your-username/hint-ai-vllm:latest .

# 2. í‘¸ì‹œ
docker login
docker push your-username/hint-ai-vllm:latest
```

### RunPodì—ì„œ (ë§¤ë²ˆ)

1. **Deploy** â†’ **Custom Image**
2. ì´ë¯¸ì§€: `your-username/hint-ai-vllm:latest`
3. GPU: RTX 4090/5090
4. Ports: `8000, 7860`
5. **Deploy** í´ë¦­!

**ë! 5-10ë¶„ í›„ ì‚¬ìš© ê°€ëŠ¥!** ğŸ‰

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **vLLM ê³µì‹ ë¬¸ì„œ**: https://docs.vllm.ai/
- **Gradio ë¬¸ì„œ**: https://www.gradio.app/docs
- **RunPod ë¬¸ì„œ**: https://docs.runpod.io/
- **Docker Hub**: https://hub.docker.com/

---

## ğŸ’¡ íŒ

### ë¹„ìš© ì ˆì•½

- **Spot Instances** ì‚¬ìš© (Community Cloud)
- ì‚¬ìš© ì•ˆ í•  ë•ŒëŠ” Pod **Stop**
- Volumeì„ ì‚¬ìš©í•´ ëª¨ë¸ ìºì‹œ ì¬ì‚¬ìš©

### ì„±ëŠ¥ ìµœì í™”

- `--gpu-memory-utilization 0.85` â†’ `0.90` (ì—¬ìœ  ìˆì„ ë•Œ)
- `--max-model-len 4096` â†’ `8192` (ê¸´ ì½”ë“œ ì²˜ë¦¬)
- SSD Volume ë§ˆìš´íŠ¸ë¡œ I/O ì†ë„ í–¥ìƒ

### ìë™í™”

- RunPod APIë¡œ Pod ìƒì„± ìë™í™”
- GitHub Actionsë¡œ Docker ì´ë¯¸ì§€ ìë™ ë¹Œë“œ/í‘¸ì‹œ
- Webhookìœ¼ë¡œ ë°°í¬ íŠ¸ë¦¬ê±°

---

**ì´ì œ RunPodì—ì„œ vLLM í…œí”Œë¦¿ì„ ì„ íƒí•˜ë“¯ì´ ì—¬ëŸ¬ë¶„ì˜ í†µí•© ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ë©´ ëì…ë‹ˆë‹¤!** ğŸš€
