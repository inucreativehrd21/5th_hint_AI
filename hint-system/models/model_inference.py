"""
ë‹¤ì–‘í•œ ëª¨ë¸ë¡œë¶€í„° íŒíŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°±ì—”ë“œ
"""
import time
import re
from typing import Dict, List, Optional
import requests
from openai import OpenAI


class ModelInference:
    """ëª¨ë¸ ì¶”ë¡  ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self, model_name: str, model_type: str):
        self.model_name = model_name
        self.model_type = model_type

    def generate_hint(self, prompt: str, max_tokens: int = 512) -> Dict:
        """
        íŒíŠ¸ ìƒì„±

        Returns:
            {
                'hint': str,  # ìƒì„±ëœ íŒíŠ¸
                'time': float,  # ì†Œìš” ì‹œê°„ (ì´ˆ)
                'model': str,  # ëª¨ë¸ ì´ë¦„
                'error': str  # ì—ëŸ¬ ë°œìƒì‹œ
            }
        """
        raise NotImplementedError


class HuggingFaceInference(ModelInference):
    """HuggingFace Transformers ê¸°ë°˜ ì¶”ë¡  (ìˆœì°¨ ë¡œë“œ + ì–‘ìí™” ì§€ì›)"""

    def __init__(self, model_name: str, use_quantization: bool = False):
        super().__init__(model_name, "huggingface")
        self.model = None
        self.tokenizer = None
        self.loaded = False
        self.use_quantization = use_quantization

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self.loaded:
            return

        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            import torch

            print(f"Loading {self.model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )

            # ì–‘ìí™” ì„¤ì • (4-bit)
            if self.use_quantization:
                print(f"  â†’ Using 4-bit quantization to save memory")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )

                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else "cpu",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )

            self.loaded = True
            print(f"âœ“ {self.model_name} loaded")
        except Exception as e:
            print(f"âœ— Failed to load {self.model_name}: {e}")
            import traceback
            traceback.print_exc()
            raise

    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.model is not None:
            print(f"Unloading {self.model_name}...")
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            self.loaded = False

            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()

            print(f"âœ“ {self.model_name} unloaded")

    def generate_hint(self, prompt: str, max_tokens: int = 150, temperature: float = 0.3) -> Dict:
        """íŒíŠ¸ ìƒì„± (temperature ì¡°ì ˆ ê°€ëŠ¥)"""
        try:
            self.load_model()

            start_time = time.time()

            # í”„ë¡¬í”„íŠ¸ í† í°í™”
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            input_length = inputs['input_ids'].shape[1]

            # ìƒì„± (temperature ì¡°ì ˆ)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,  # 150 í† í°ìœ¼ë¡œ ì œí•œ (1-2 ë¬¸ì¥)
                temperature=temperature,  # UIì—ì„œ ì¡°ì ˆ ê°€ëŠ¥
                top_p=0.9,  # ì•ˆì •ì ì¸ ìƒ˜í”Œë§
                do_sample=True,
                repetition_penalty=1.2,  # ë°˜ë³µ ì–µì œ
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

            # ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_ids = outputs[0][input_length:]
            raw_output = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

            # ê¹”ë”í•œ íŒíŠ¸ë§Œ ì¶”ì¶œ (í›„ì²˜ë¦¬)
            hint = self._extract_hint_from_output(raw_output)

            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
            if not hint or len(hint) < 10:
                hint = "(ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)"

            elapsed = time.time() - start_time

            return {
                'hint': hint,
                'time': elapsed,
                'model': self.model_name,
                'error': None
            }

        except Exception as e:
            import traceback
            traceback.print_exc()
            return {
                'hint': '',
                'time': 0,
                'model': self.model_name,
                'error': str(e)
            }

    def _extract_hint_from_output(self, raw_output: str) -> str:
        """ìƒì„±ëœ ì¶œë ¥ì—ì„œ ì˜ë¯¸ìˆëŠ” íŒíŠ¸ë§Œ ì¶”ì¶œ (ê°•í™”ëœ í•„í„°ë§)"""
        import re

        # ë””ë²„ê¹…: ì›ë³¸ ì¶œë ¥ í™•ì¸
        print(f"\n[DEBUG] ì›ë³¸ ì¶œë ¥: {raw_output[:200]}")

        clean = raw_output.strip()

        # ì˜ì–´ ì¶œë ¥ í•„í„°ë§ (í•œêµ­ì–´ë§Œ í—ˆìš©)
        korean_chars = len(re.findall(r'[ê°€-í£]', clean))
        total_chars = len(clean.replace(' ', ''))
        if total_chars > 0 and korean_chars / total_chars < 0.3:
            # í•œêµ­ì–´ê°€ 30% ë¯¸ë§Œì´ë©´ ì˜ì–´ë¡œ íŒë‹¨
            print(f"[DEBUG] ì˜ì–´ ì¶œë ¥ í•„í„°ë§: {clean[:50]}...")
            return "(ëª¨ë¸ì´ í•œêµ­ì–´ íŒíŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)"

        # ë©”íƒ€ í‘œí˜„ ê°•ë ¥ í•„í„°ë§ (í”„ë¡¬í”„íŠ¸ ë°˜ë³µ)
        meta_patterns = [
            r'^í•™ìƒì´.*ì§ˆë¬¸.*ì‘ì„±',
            r'^ìœ„.*ì›ì¹™.*ë”°ë¼',
            r'^ìœ„.*ì„¤ëª….*ë”°ë¼',
            r'^ë‹¤ìŒ.*ì†Œí¬ë¼í…ŒìŠ¤',
            r'^ìµœì¢….*ë‹µ:',
            r'^ë‹µë³€:',
            r'^ì§ˆë¬¸:',
            r'^\[.*\]',  # [íŒíŠ¸], [ë¶„ì„] ê°™ì€ íƒœê·¸
        ]

        for pattern in meta_patterns:
            if re.match(pattern, clean, re.IGNORECASE):
                # ë©”íƒ€ í‘œí˜„ ë°œê²¬ ì‹œ ë‹¤ìŒ ë¬¸ì¥ë¶€í„° ì°¾ê¸°
                sentences = re.split(r'[.!?\n]+', clean)
                for s in sentences[1:]:
                    s = s.strip()
                    if len(s) > 15 and not any(re.match(p, s, re.IGNORECASE) for p in meta_patterns):
                        clean = s
                        break

        # ì—‰ëš±í•œ ì£¼ì œ í•„í„°ë§
        irrelevant_keywords = [
            'ì§ê°', 'ì‚¼ê°í˜•', 'ì›', 'ë©´ì ', 'ë‘˜ë ˆ', 'í”¼íƒ€ê³ ë¼ìŠ¤',
            'ì œê³±ê·¼', 'ë£¨íŠ¸', 'ê°ë„', 'ë¼ë””ì•ˆ', 'ì‚¼ê°í•¨ìˆ˜',
        ]

        # ë¬¸ì¥ë³„ë¡œ ì²´í¬
        sentences = re.split(r'[.!?\n]+', clean)
        valid_sentences = []

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 15:
                continue

            # ì—‰ëš±í•œ í‚¤ì›Œë“œ ì²´í¬
            if any(keyword in sentence for keyword in irrelevant_keywords):
                print(f"[DEBUG] ë¬´ê´€í•œ ë‚´ìš© í•„í„°ë§: {sentence[:50]}...")
                continue

            # ë©”íƒ€ ì–¸ê¸‰ ë° ì´ìƒí•œ ì‹œì‘ ì œê±°
            bad_starts = [
                'ì–´ë–¤ ì ì„', 'ë”°ë¼ì„œ', 'í•™ìƒ ì½”ë“œ', 'ì •ë‹µ ì½”ë“œ', 'ì´ ì½”ë“œ', 'ìœ„ ì½”ë“œ', 'ì•„ë˜ ì½”ë“œ',
                'í•™ìƒì´ ë‹¤ìŒ', 'ìœ„ ì›ì¹™', 'ê·¸ëŸ¬ë‚˜', 'ê²°êµ­', 'ê·¸ëŸ¼', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ',
                'ì •ë‹µì½”ë“œ', 'ì •ë‹µì—ì„œ', 'ì½”ë“œì—ì„œ', 'ìœ„ì˜ ì„¤ëª…', 'ìœ„ ì„¤ëª…', 'ìœ„ ë‚´ìš©'
            ]
            if any(sentence.startswith(bad) for bad in bad_starts):
                print(f"[DEBUG] ë©”íƒ€ ì–¸ê¸‰/ì´ìƒí•œ ì‹œì‘ í•„í„°ë§: {sentence[:50]}...")
                continue

            # ì •ë‹µ ì½”ë“œ ì§ì ‘ ì–¸ê¸‰ í•„í„°ë§ (ê°•í™”)
            bad_references = ['ì •ë‹µ', 'ì •ë‹µì½”ë“œ', 'ì™„ì„±ëœ ì½”ë“œ', 'ì™„ì„± ì½”ë“œ', 'ìœ„ ì½”ë“œ', 'ì°¸ê³  ì½”ë“œ']
            if any(ref in sentence for ref in bad_references):
                print(f"[DEBUG] ì •ë‹µ/ì™„ì„± ì½”ë“œ ì–¸ê¸‰ í•„í„°ë§: {sentence[:50]}...")
                continue

            # ì§ì ‘ ë‹µ ìš”êµ¬ í•„í„°ë§ (ì‹ ê·œ)
            direct_answer_patterns = ['ì„¤ëª…í•´ì¤˜', 'ì•Œë ¤ì¤˜', 'ê°€ë¥´ì³ì¤˜', 'ë§í•´ì¤˜', 'ì–´ë–»ê²Œ ì‘ë™']
            if any(pattern in sentence for pattern in direct_answer_patterns):
                print(f"[DEBUG] ì§ì ‘ ë‹µ ìš”êµ¬ í•„í„°ë§: {sentence[:50]}...")
                continue

            # í•¨ìˆ˜ëª…/ë³€ìˆ˜ëª… ì–¸ê¸‰ í•„í„°ë§ (ì‹ ê·œ - ë§¤ìš° ì¤‘ìš”!)
            # snake_caseë‚˜ camelCase íŒ¨í„´ ê°ì§€ (ì¼ë°˜ ë‹¨ì–´ ì œì™¸)
            import re
            # 2ê°œ ì´ìƒì˜ ë‹¨ì–´ê°€ _ë¡œ ì—°ê²°ë˜ê±°ë‚˜, ì†Œë¬¸ì+ëŒ€ë¬¸ì ì¡°í•©
            function_name_pattern = r'\b[a-z]+_[a-z]+\b|[a-z]+[A-Z][a-z]+'
            if re.search(function_name_pattern, sentence):
                # ì¼ë°˜ì ì¸ ë‹¨ì–´ ì˜ˆì™¸ ì²˜ë¦¬
                common_words = ['input', 'print', 'range', 'split', 'append']
                suspicious_names = re.findall(function_name_pattern, sentence)
                if any(name not in common_words for name in suspicious_names):
                    print(f"[DEBUG] í•¨ìˆ˜ëª…/ë³€ìˆ˜ëª… ì–¸ê¸‰ í•„í„°ë§: {sentence[:50]}...")
                    continue

            # í•™ìƒ ì½”ë“œ ë°˜ë³µ í•„í„°ë§ (ì½”ë“œ ìì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥)
            if 'int(input())' in sentence or 'print(' in sentence:
                # ë‹¨, ì§ˆë¬¸ í˜•íƒœëŠ” í—ˆìš©
                if not sentence.endswith('?'):
                    print(f"[DEBUG] ì½”ë“œ ë°˜ë³µ í•„í„°ë§: {sentence[:50]}...")
                    continue

            # ë§‰ì—°í•œ ì§€ì‹œë¬¸ ë° ì´ìƒí•œ í‘œí˜„ í•„í„°ë§ (ê°•í™”)
            vague_patterns = [
                'í™•ì¸í•˜ì„¸ìš”', 'ìƒê°í•´ë³´ì„¸ìš”', 'ê³ ë ¤í•˜ì„¸ìš”', 'ê²€í† í•˜ì„¸ìš”',
                'ì‘ì„±í•˜ì„¸ìš”', 'êµ¬í˜„í•˜ì„¸ìš”', 'ì½”ë“œë¥¼ ì™„ì„±',
                'ì–´ë–¤ ë¶€ë¶„ì´ ì˜ëª»', 'ë¬´ì—‡ì´ ë¬¸ì œ', 'ì–´ë””ê°€ í‹€ë ¸',
                'ëŒ€í™”í•˜ê¸° ìœ„í•´', 'ìš”ì²­í•˜ëŠ”ì§€',
                'ë¬¼ìœ¼ì‹œì£ ', 'ë¬¼ìœ¼ì„¸ìš”', 'í•´ì£¼ì„¸ìš”', 'í•´ì£¼ì‹œ', 'ì£¼ì„¸ìš”',
                'ë‹¤ì‹œ í•œë²ˆ', 'ë„¤, ì´í•´', 'ê·¸ë ‡ë‹¤ë©´', 'ê·¸ëŸ¼ ì§€ê¸ˆë¶€í„°',
                'ì˜ëª»í–ˆë‹¤ê³ ', 'í‹€ë ¸ë‹¤ê³ ', 'ìƒê°í–ˆìŠµë‹ˆë‹¤', 'ìƒê°ë©ë‹ˆë‹¤',
                'í•µì‹¬ì ì¸ ê³¼ì •', 'ì–´ë–¤ ë¶€ë¶„ì„'
            ]
            if any(pattern in sentence for pattern in vague_patterns):
                print(f"[DEBUG] ë§‰ì—°í•œ ì§€ì‹œë¬¸ í•„í„°ë§: {sentence[:50]}...")
                continue

            # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ í•„í„°ë§ (20 â†’ 30ìë¡œ ìƒí–¥)
            if len(sentence) < 30:
                print(f"[DEBUG] ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ í•„í„°ë§: {sentence}")
                continue

            # ì¶”ìƒì ì¸ í‘œí˜„ë§Œ ìˆëŠ” íŒíŠ¸ í•„í„°ë§ (ì‹ ê·œ)
            abstract_only = [
                'ê²°ê³¼ ì¶œë ¥', 'ì½”ë“œ ì‘ì„±', 'ë¡œì§ êµ¬í˜„', 'ì•Œê³ ë¦¬ì¦˜ ì‘ì„±',
                'ë¬¸ì œ í•´ê²°', 'ë°©ë²• ì°¾ê¸°', 'êµ¬ì¡° ë§Œë“¤ê¸°'
            ]
            # ì§ˆë¬¸ì— êµ¬ì²´ì ì¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê±°ë¶€
            has_concrete = any(keyword in sentence for keyword in [
                'í•¨ìˆ˜', 'ë°˜ë³µë¬¸', 'for', 'while', 'def', 'ì¡°ê±´ë¬¸', 'if',
                'ë³€ìˆ˜', 'ë¦¬ìŠ¤íŠ¸', 'ë°°ì—´', 'ì…ë ¥', 'input', 'ì¶œë ¥', 'print',
                'append', 'range', 'len', 'max', 'min', 'sum', 'sorted'
            ])
            if any(abstract in sentence for abstract in abstract_only) and not has_concrete:
                print(f"[DEBUG] ì¶”ìƒì  í‘œí˜„ë§Œ ìˆëŠ” íŒíŠ¸ í•„í„°ë§: {sentence[:50]}...")
                continue

            valid_sentences.append(sentence)

        # ìœ íš¨í•œ ë¬¸ì¥ ì¤‘ì—ì„œ ì§ˆë¬¸ ìš°ì„  ì¶”ì¶œ (ë¬¼ìŒí‘œë¡œ ëë‚˜ëŠ” ê²ƒ)
        for sentence in valid_sentences:
            if sentence.endswith('?'):
                hint = sentence.strip()
                print(f"[DEBUG] ìœ íš¨í•œ ì§ˆë¬¸ ë°œê²¬: {hint}")
                return hint

        # ë¬¼ìŒí‘œê°€ ì¤‘ê°„ì— ìˆëŠ” ê²½ìš°
        for sentence in valid_sentences:
            if '?' in sentence:
                hint = sentence.split('?')[0].strip() + '?'
                print(f"[DEBUG] ë¬¼ìŒí‘œ í¬í•¨ ë¬¸ì¥: {hint}")
                return hint

        # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ìœ íš¨í•œ ë¬¸ì¥
        if valid_sentences:
            hint = valid_sentences[0].strip()
            print(f"[DEBUG] ì²« ìœ íš¨ ë¬¸ì¥: {hint}")
            return hint

        # ëª¨ë“  í•„í„°ë§ ì‹¤íŒ¨ - ê¸°ë³¸ ë©”ì‹œì§€
        print(f"[DEBUG] ìœ íš¨í•œ íŒíŠ¸ ì—†ìŒ")
        return "(ëª¨ë¸ì´ ì ì ˆí•œ íŒíŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)"


class VLLMInference(ModelInference):
    """
    vLLM OpenAI í˜¸í™˜ API ê¸°ë°˜ ì¶”ë¡ 

    íŠ¹ì§•:
    - 15-24ë°° ë¹ ë¥¸ ì¶”ë¡  ì†ë„ (HuggingFace ëŒ€ë¹„)
    - Continuous batchingìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”
    - PagedAttentionìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    - Retry ë¡œì§ ë° connection pooling
    - Health check ë° timeout ì„¤ì •
    """

    def __init__(
        self,
        model_name: str,
        base_url: str = "http://localhost:8000/v1",
        timeout: int = 60,
        max_retries: int = 3,
        retry_delay: float = 1.0
    ):
        super().__init__(model_name, "vllm")
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (vLLM ì„œë²„ìš©)
        # openai 1.3.x+ ë²„ì „ì—ì„œëŠ” timeout, max_retries ì¸ì ì§€ì›
        self.client = OpenAI(
            base_url=base_url,
            api_key="dummy"  # vLLMì€ API keyê°€ í•„ìš”ì—†ì§€ë§Œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ í•„ìš”
        )
        
        # timeoutê³¼ max_retriesëŠ” í´ë¼ì´ì–¸íŠ¸ ì†ì„±ìœ¼ë¡œ ì„¤ì •
        self.client.timeout = timeout
        self.client.max_retries = max_retries

        # ì„œë²„ ì—°ê²° í™•ì¸
        self._check_server_health()
    
    def _extract_output_from_cot(self, hint: str) -> str:
        """
        CoT íŒŒì‹±: <output> íƒœê·¸ì—ì„œ ì‹¤ì œ íŒíŠ¸ ì¶”ì¶œ
        
        Args:
            hint: ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µ (CoT í¬í•¨ ê°€ëŠ¥)
        
        Returns:
            str: íŒŒì‹±ëœ íŒíŠ¸ (íƒœê·¸ ì œê±°)
        """
        # <output>...</output> íƒœê·¸ ì°¾ê¸° (ë©€í‹°ë¼ì¸ ì§€ì›)
        output_match = re.search(r'<output>(.*?)</output>', hint, re.DOTALL | re.IGNORECASE)
        
        if output_match:
            extracted = output_match.group(1).strip()
            print(f"[CoT] <output> íƒœê·¸ ë°œê²¬ - íŒŒì‹± ì™„ë£Œ ({len(extracted)} chars)")
            
            # <thinking> ë¶€ë¶„ë„ ë¡œê·¸ë¡œ ë‚¨ê¸°ê¸° (ë””ë²„ê¹…ìš©)
            thinking_match = re.search(r'<thinking>(.*?)</thinking>', hint, re.DOTALL | re.IGNORECASE)
            if thinking_match:
                thinking_content = thinking_match.group(1).strip()
                print(f"[CoT] <thinking> ë‚´ìš© ({len(thinking_content)} chars):")
                print(f"  {thinking_content[:200]}...")
            
            return extracted
        else:
            # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (fallback)
            print(f"[CoT] <output> íƒœê·¸ ì—†ìŒ - ì›ë³¸ ì‘ë‹µ ì‚¬ìš©")
            return hint

    def _check_server_health(self) -> bool:
        """vLLM ì„œë²„ health check"""
        try:
            # OpenAI APIì˜ models ì—”ë“œí¬ì¸íŠ¸ë¡œ ì„œë²„ ìƒíƒœ í™•ì¸
            models = self.client.models.list()
            available_models = [model.id for model in models.data]

            if available_models:
                print(f"âœ“ vLLM server connected at {self.base_url}")
                print(f"  Available models: {', '.join(available_models)}")
                
                # ì„œë²„ì— ìš”ì²­í•œ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ìë™ ì„¤ì •
                if self.model_name not in available_models:
                    old_name = self.model_name
                    self.model_name = available_models[0]
                    print(f"âš  Model '{old_name}' not found. Using '{self.model_name}' instead.")
                
                return True
            else:
                print(f"âš  vLLM server at {self.base_url} has no models loaded")
                return False

        except Exception as e:
            print(f"âœ— vLLM server at {self.base_url} is not reachable: {e}")
            print(f"  Please ensure vLLM server is running:")
            print(f"    python vllm_server.py --model {self.model_name}")
            return False

    def generate_hint(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        system_prompt: Optional[str] = None
    ) -> Dict:
        """
        íŒíŠ¸ ìƒì„± (vLLM ì„œë²„ ì‚¬ìš©)

        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„ (0.0-2.0, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
            top_p: Nucleus sampling threshold
            frequency_penalty: ë°˜ë³µ ì–µì œ (-2.0 ~ 2.0)
            presence_penalty: ì£¼ì œ ë‹¤ì–‘ì„± (-2.0 ~ 2.0)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            {
                'hint': str,  # ìƒì„±ëœ íŒíŠ¸
                'time': float,  # ì†Œìš” ì‹œê°„ (ì´ˆ)
                'model': str,  # ëª¨ë¸ ì´ë¦„
                'error': str,  # ì—ëŸ¬ ë°œìƒì‹œ
                'tokens': int,  # ìƒì„±ëœ í† í° ìˆ˜
                'finish_reason': str  # ì™„ë£Œ ì´ìœ  (stop, length, etc.)
            }
        """
        # system_prompt ê°•í™”: í˜•ì‹ ì¤€ìˆ˜ ê°•ì œ
        if system_prompt is None:
            system_prompt = """ë‹¹ì‹ ì€ Python ì½”ë”© êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš”: ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:

1. **ì‚¬ìš©ì ë©”ì‹œì§€ ì²« ì¤„ì˜ í˜„ì¬ íŒíŠ¸ ë ˆë²¨ì„ í™•ì¸**í•˜ì„¸ìš”
   - "ì´ˆê¸‰ (Novice)" â†’ ğŸ’¡ğŸ“ğŸ’»ğŸ¯ í˜•ì‹ ì‚¬ìš©
   - "ì¤‘ê¸‰ (Intermediate)" â†’ ğŸ§ ğŸ“ŠğŸ’¾ í˜•ì‹ ì‚¬ìš©
   - "ê³ ê¸‰ (Advanced)" â†’ ğŸ”â“ í˜•ì‹ ì‚¬ìš©

2. **ì´ì „ íŒíŠ¸ í˜•ì‹ê³¼ ê´€ê³„ì—†ì´**, í˜„ì¬ ì§€ì •ëœ ë ˆë²¨ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”

3. "í•™ìƒì€ ~", "ì§„ë‹¨ ê²°ê³¼ì—ì„œ ~", "1ë‹¨ê³„:", "2ë‹¨ê³„:" ê°™ì€ **ë¶„ì„/ì„¤ëª… ì ˆëŒ€ ê¸ˆì§€**

4. í•™ìƒ ì½”ë“œë¥¼ ë¶„ì„í•˜ë˜, **ë¶„ì„ ë‚´ìš©ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”**

5. **ì§€ì •ëœ í˜•ì‹ ì™¸ì˜ ëª¨ë“  ë‚´ìš© ìƒëµ**í•˜ì„¸ìš”

6. ì¶œë ¥ ì‹œì‘ì€ ë°˜ë“œì‹œ í•´ë‹¹ ë ˆë²¨ì˜ ì²« ì´ëª¨ì§€ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤
   - ì´ˆê¸‰: ğŸ’¡ë¡œ ì‹œì‘
   - ì¤‘ê¸‰: ğŸ§ ë¡œ ì‹œì‘
   - ê³ ê¸‰: ğŸ”ë¡œ ì‹œì‘"""

        try:
            start_time = time.time()
            
            # ë””ë²„ê¹…: ìš”ì²­ ì •ë³´ ë¡œê·¸
            print(f"\n[VLLMInference] ìš”ì²­ ì‹œì‘")
            print(f"  Model: {self.model_name}")
            print(f"  Temperature: {temperature}")
            print(f"  Max tokens: {max_tokens}")
            print(f"  System prompt: {system_prompt[:100]}...")
            print(f"  User prompt length: {len(prompt)} chars")
            print(f"  User prompt preview: {prompt[:200]}...")

            # OpenAI Chat Completions API í˜¸ì¶œ (vLLMì´ í˜¸í™˜ ì§€ì›)
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                n=1,  # ìƒì„±í•  ì‘ë‹µ ê°œìˆ˜
                stream=False  # Streaming ë¹„í™œì„±í™” (í•„ìš”ì‹œ ë³„ë„ ë©”ì„œë“œ êµ¬í˜„ ê°€ëŠ¥)
            )

            hint = response.choices[0].message.content.strip()
            elapsed = time.time() - start_time

            # CoT íŒŒì‹±: <output> íƒœê·¸ ì¶”ì¶œ (ìˆìœ¼ë©´)
            hint = self._extract_output_from_cot(hint)

            # í† í° ì‚¬ìš©ëŸ‰ ë° ì™„ë£Œ ì´ìœ  ì¶”ì¶œ
            tokens_used = response.usage.completion_tokens if response.usage else 0
            finish_reason = response.choices[0].finish_reason
            
            # ë””ë²„ê¹…: ì‘ë‹µ ì •ë³´ ë¡œê·¸
            print(f"[VLLMInference] ì‘ë‹µ ì™„ë£Œ")
            print(f"  Elapsed time: {elapsed:.2f}s")
            print(f"  Tokens generated: {tokens_used}")
            print(f"  Finish reason: {finish_reason}")
            print(f"  Hint length: {len(hint)} chars")
            print(f"  Hint preview: {hint[:300]}...")

            return {
                'hint': hint,
                'time': elapsed,
                'model': self.model_name,
                'error': None,
                'tokens': tokens_used,
                'finish_reason': finish_reason
            }

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)

            # ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€
            if "Connection" in error_msg or "timeout" in error_msg.lower():
                error_msg = f"vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ({self.base_url}). ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
            elif "model" in error_msg.lower():
                error_msg = f"ëª¨ë¸ '{self.model_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."

            print(f"âœ— VLLMInference error: {error_msg}")

            return {
                'hint': '',
                'time': elapsed,
                'model': self.model_name,
                'error': error_msg,
                'tokens': 0,
                'finish_reason': 'error'
            }

    def generate_hint_stream(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        system_prompt: Optional[str] = None
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŒíŠ¸ ìƒì„± (ì‹¤ì‹œê°„ ì‘ë‹µ)

        Yields:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬
        """
        if system_prompt is None:
            system_prompt = "ë‹¹ì‹ ì€ ì†Œí¬ë¼í…ŒìŠ¤ì‹ í”„ë¡œê·¸ë˜ë° ë©˜í† ì…ë‹ˆë‹¤."

        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            yield f"\n[Error: {str(e)}]"


class OllamaInference(ModelInference):
    """Ollama API ê¸°ë°˜ ì¶”ë¡ """

    def __init__(self, model_name: str, base_url: str = "http://localhost:11434"):
        super().__init__(model_name, "ollama")
        self.base_url = base_url

    def generate_hint(self, prompt: str, max_tokens: int = 512) -> Dict:
        """íŒíŠ¸ ìƒì„±"""
        try:
            start_time = time.time()

            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": max_tokens
                    }
                },
                timeout=120
            )

            response.raise_for_status()
            hint = response.json()['response'].strip()
            elapsed = time.time() - start_time

            return {
                'hint': hint,
                'time': elapsed,
                'model': self.model_name,
                'error': None
            }

        except Exception as e:
            return {
                'hint': '',
                'time': 0,
                'model': self.model_name,
                'error': str(e)
            }


class ModelManager:
    """ì—¬ëŸ¬ ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì € (ìˆœì°¨ ë¡œë“œ ì§€ì›)"""

    def __init__(self, sequential_load: bool = True):
        self.models: Dict[str, ModelInference] = {}
        self.sequential_load = sequential_load  # ìˆœì°¨ ë¡œë“œ ì—¬ë¶€

    def add_huggingface_model(self, name: str, model_path: str, use_quantization: bool = False):
        """HuggingFace ëª¨ë¸ ì¶”ê°€"""
        self.models[name] = HuggingFaceInference(model_path, use_quantization)

    def add_vllm_model(self, name: str, model_path: str, base_url: str):
        """vLLM ëª¨ë¸ ì¶”ê°€"""
        self.models[name] = VLLMInference(model_path, base_url)

    def add_ollama_model(self, name: str, model_name: str, base_url: str = "http://localhost:11434"):
        """Ollama ëª¨ë¸ ì¶”ê°€"""
        self.models[name] = OllamaInference(model_name, base_url)

    def generate_hints_from_all(self, prompt: str) -> Dict[str, Dict]:
        """ëª¨ë“  ëª¨ë¸ë¡œë¶€í„° íŒíŠ¸ ìƒì„± (ìºì‹œ ìœ ì§€ ë°©ì‹)"""
        results = {}

        for name, model in self.models.items():
            print(f"Generating hint from {name}...")

            # íŒíŠ¸ ìƒì„± (ëª¨ë¸ì€ ìë™ìœ¼ë¡œ ë¡œë“œë˜ê³  ìºì‹œë¨)
            results[name] = model.generate_hint(prompt)

            # ìˆœì°¨ ë¡œë“œ ëª¨ë“œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ìºì‹œ ìœ ì§€)
            # if self.sequential_load and isinstance(model, HuggingFaceInference):
            #     model.unload_model()
            #     print(f"  â†’ Memory freed for next model\n")

        return results

    def generate_hints_from_selected(self, prompt: str, selected_names: List[str], temperature: float = 0.3) -> Dict[str, Dict]:
        """ì„ íƒëœ ëª¨ë¸ë“¤ë¡œë¶€í„°ë§Œ íŒíŠ¸ ìƒì„± (temperature ì „ë‹¬)"""
        results = {}

        for name in selected_names:
            if name in self.models:
                print(f"Generating hint from {name}... (temp={temperature})")
                model = self.models[name]
                results[name] = model.generate_hint(prompt, temperature=temperature)
            else:
                print(f"âš ï¸  Model {name} not found, skipping...")

        return results

    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        return list(self.models.keys())
