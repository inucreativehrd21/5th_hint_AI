# ============================================================================
# Hint Generation System - 환경 변수 설정
# ============================================================================
# 사용법: 이 파일을 .env로 복사하고 필요한 값을 수정하세요
#        cp .env.example .env
# ============================================================================

# ============================================================================
# vLLM 서버 설정 (Docker 기반 배포)
# ============================================================================

# 사용할 모델 (HuggingFace 모델 ID)
# 추천 모델:
#   - Qwen/Qwen2.5-Coder-7B-Instruct (7B, 빠른 추론, 권장)
#   - deepseek-ai/deepseek-coder-6.7b-instruct (6.7B)
#   - codellama/CodeLlama-7b-Instruct-hf (7B)
VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct

# GPU 메모리 사용률 (0.0 ~ 1.0)
# 0.85 권장 (안정성과 성능의 균형, OOM 방지)
VLLM_GPU_MEMORY_UTILIZATION=0.85

# 최대 시퀀스 길이 (토큰 수)
# 4096 권장 (코드 힌트 생성에 충분)
VLLM_MAX_MODEL_LEN=4096

# Tensor Parallel Size (다중 GPU 사용 시)
# 1 = 단일 GPU (기본값)
# 2 = 2개 GPU (A100 80GB x2 등)
VLLM_TENSOR_PARALLEL_SIZE=1

# 모델 데이터 타입
# auto = 자동 선택 (권장)
# half, float16 = FP16
# bfloat16 = BF16 (A100 등 지원 GPU)
VLLM_DTYPE=auto

# HuggingFace 토큰 (private 모델 사용 시 필요)
# https://huggingface.co/settings/tokens 에서 발급
# public 모델만 사용하면 불필요
HUGGING_FACE_HUB_TOKEN=

# ============================================================================
# Gradio 애플리케이션 설정
# ============================================================================

# vLLM 서버 URL
# Docker Compose: http://vllm-server:8000/v1 (자동 설정)
# 로컬 테스트: http://localhost:8000/v1
VLLM_SERVER_URL=http://vllm-server:8000/v1

# Gradio 포트
GRADIO_PORT=7860

# 기본 Temperature 설정 (0.0 ~ 2.0)
# 0.7 = 균형잡힌 창의성 (권장)
DEFAULT_TEMPERATURE=0.7

# ============================================================================
# 데이터 파일 경로
# ============================================================================

# 문제 데이터 JSON 파일 경로
DATA_FILE_PATH=data/problems_multi_solution.json

# 평가 결과 저장 디렉토리
EVALUATION_RESULTS_DIR=evaluation/results

# ============================================================================
# 로깅 설정
# ============================================================================

# 로그 파일 경로
LOG_FILE=logs/app.log

# 로그 레벨 (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ============================================================================
# 크롤러 설정 (선택 사항)
# ============================================================================

# 크롤러 출력 디렉토리
CRAWLER_OUTPUT_DIR=crawler/output

# Solved.ac API 키 (선택 사항)
SOLVED_AC_API_KEY=

LOG_FILE=logs/app.log

# ============================================================================
# Advanced Settings
# ============================================================================
# Enable/disable sequential model loading to save memory
SEQUENTIAL_MODEL_LOAD=true

# Maximum tokens to generate for hints
MAX_HINT_TOKENS=512

# Number of hints to generate per request
NUM_HINTS_PER_REQUEST=1
