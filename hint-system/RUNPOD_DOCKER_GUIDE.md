# RunPod vLLM Docker ì‹¤í–‰ ê°€ì´ë“œ (ë©˜í† ë‹˜ í”¼ë“œë°± ë°˜ì˜)

## ğŸ¯ í•µì‹¬ ê°œë…

**ë©˜í† ë‹˜ í”¼ë“œë°±:**
> DockerHubì—ì„œ ìµœì‹  vllm ê´€ë ¨ ì´ë¯¸ì§€ê°€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë˜ê³  ìˆê³ , ê·¸ ì´ë¯¸ì§€ë¥¼ ë‹¨ìˆœíˆ ë‚´ë ¤ë°›ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤! ì „ë¶€ ë”¸ê¹ì…ë‹ˆë‹¤ ã…ã…

**ë³€ê²½ ì‚¬í•­:**
- âŒ **Before**: RunPod ì ‘ì† â†’ vllm pip ì„¤ì¹˜ â†’ `python vllm_server.py` ì‹¤í–‰
- âœ… **After**: DockerHub ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ â†’ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ â†’ ì£¼ì†Œë¡œ í†µì‹ 

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Docker ìˆëŠ” ê²½ìš°)

### ì „ì œ ì¡°ê±´
RunPodì—ì„œ **Docker ì§€ì› í…œí”Œë¦¿** ì‚¬ìš©:
- `runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04`
- ë˜ëŠ” Base í…œí”Œë¦¿

### ì›í´ë¦­ ì‹¤í–‰

```bash
cd /workspace
git clone https://github.com/inucreativehrd21/5th_project_mvp.git
cd 5th_project_mvp/hint-system
bash runpod_docker_simple.sh
```

**ë!** 5ë¶„ì´ë©´ ì‹¤í–‰ë©ë‹ˆë‹¤.

---

## ğŸ“‹ ì‘ë™ ì›ë¦¬

### ê¸°ì¡´ ë°©ì‹ (Offline Serving)

```
RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  â†“
SSH ì ‘ì†
  â†“
pip install vllm  (5-10ë¶„)
  â†“
python vllm_server.py  (ìˆ˜ë™ ì‹¤í–‰)
  â†“
python app.py  (ë³„ë„ í„°ë¯¸ë„)
```

### ìƒˆë¡œìš´ ë°©ì‹ (Docker ì´ë¯¸ì§€)

```
RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Docker ì§€ì› í…œí”Œë¦¿)
  â†“
docker pull vllm/vllm-openai:latest  (ë”¸ê¹!)
  â†“
docker-compose up -d  (ìë™ ì‹¤í–‰)
  â†“
âœ… vLLM ì„œë²„ê°€ ì´ë¯¸ ì„œë¹™ ì¤‘!
âœ… Gradio UIê°€ ì£¼ì†Œë¡œ í†µì‹ !
```

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### Docker Compose êµ¬ì¡°

```yaml
services:
  vllm-server:  # vLLM ê³µì‹ ì´ë¯¸ì§€
    image: vllm/vllm-openai:latest  # â† ë”¸ê¹!
    ports: ["8000:8000"]
    command: >
      --model Qwen/Qwen2.5-Coder-7B-Instruct
      --host 0.0.0.0
      --port 8000
  
  gradio-ui:  # ì»¤ìŠ¤í…€ Gradio ì•±
    build: Dockerfile.gradio
    ports: ["7860:7860"]
    environment:
      - VLLM_API_BASE=http://vllm-server:8000/v1  # â† ì£¼ì†Œë¡œ í†µì‹ !
```

### í†µì‹  íë¦„

```
ì‚¬ìš©ì
  â†“ (ì›¹ ë¸Œë¼ìš°ì €)
Gradio UI (7860 í¬íŠ¸)
  â†“ (HTTP API í˜¸ì¶œ)
vLLM Server (8000 í¬íŠ¸)
  â†“ (GPU ì¶”ë¡ )
ì‘ë‹µ ë°˜í™˜
```

---

## ğŸ“¦ íŒŒì¼ êµ¬ì¡°

### ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼

```
hint-system/
â”œâ”€â”€ docker-compose.runpod.yml    # Docker Compose ì„¤ì • (í•µì‹¬!)
â”œâ”€â”€ Dockerfile.gradio             # Gradio UI ì´ë¯¸ì§€
â”œâ”€â”€ runpod_docker_simple.sh       # ì›í´ë¦­ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ RUNPOD_DOCKER_GUIDE.md        # ì´ ë¬¸ì„œ
```

### ê¸°ì¡´ íŒŒì¼ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)

```
hint-system/
â”œâ”€â”€ app.py                        # Gradio UI ì½”ë“œ
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_inference.py        # vLLM API í˜¸ì¶œ ë¡œì§
â”‚   â”œâ”€â”€ code_analyzer.py
â”‚   â”œâ”€â”€ adaptive_prompt.py
â”‚   â””â”€â”€ hint_validator.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ problems_multi_solution.json
â””â”€â”€ requirements.txt
```

---

## ğŸ”§ ìƒì„¸ ì„¤ì •

### docker-compose.runpod.yml

```yaml
version: '3.8'

services:
  vllm-server:
    image: vllm/vllm-openai:latest  # DockerHub ê³µì‹ ì´ë¯¸ì§€
    container_name: vllm-hint-server
    ports:
      - "8000:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN:-}  # HuggingFace í† í° (ì„ íƒ)
    command: >
      --model Qwen/Qwen2.5-Coder-7B-Instruct
      --host 0.0.0.0
      --port 8000
      --dtype auto
      --max-model-len 4096
      --gpu-memory-utilization 0.85
      --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  gradio-ui:
    build:
      context: .
      dockerfile: Dockerfile.gradio
    container_name: hint-gradio-ui
    ports:
      - "7860:7860"
    environment:
      - VLLM_API_BASE=http://vllm-server:8000/v1  # vLLM ì„œë²„ ì£¼ì†Œ
      - VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct
    depends_on:
      vllm-server:
        condition: service_healthy  # vLLM ì¤€ë¹„ í›„ ì‹œì‘
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped
```

### Dockerfile.gradio

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì•± ì½”ë“œ
COPY . .

EXPOSE 7860

CMD ["python", "app.py"]
```

---

## ğŸ® ì‚¬ìš© ë°©ë²•

### 1. ì„œë¹„ìŠ¤ ì‹œì‘

```bash
bash runpod_docker_simple.sh
```

ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ:

```bash
docker-compose -f docker-compose.runpod.yml up -d
```

### 2. ë¡œê·¸ í™•ì¸

```bash
# vLLM ì„œë²„ ë¡œê·¸
docker-compose -f docker-compose.runpod.yml logs -f vllm-server

# Gradio UI ë¡œê·¸
docker-compose -f docker-compose.runpod.yml logs -f gradio-ui

# ëª¨ë“  ë¡œê·¸
docker-compose -f docker-compose.runpod.yml logs -f
```

### 3. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸

```bash
docker-compose -f docker-compose.runpod.yml ps
```

### 4. Health Check

```bash
# vLLM ì„œë²„
curl http://localhost:8000/health

# ëª¨ë¸ ì •ë³´
curl http://localhost:8000/v1/models

# Gradio UI
curl http://localhost:7860
```

### 5. ì„œë¹„ìŠ¤ ì¤‘ì§€

```bash
docker-compose -f docker-compose.runpod.yml down
```

### 6. ì„œë¹„ìŠ¤ ì¬ì‹œì‘

```bash
docker-compose -f docker-compose.runpod.yml restart
```

---

## ğŸŒ ì™¸ë¶€ ì ‘ì† (RunPod í¬íŠ¸ ë…¸ì¶œ)

### RunPod ëŒ€ì‹œë³´ë“œ ì„¤ì •

1. Pod ìƒì„¸ í˜ì´ì§€ ì´ë™
2. "Connect" â†’ "TCP Port Mappings"
3. í¬íŠ¸ ì¶”ê°€:
   ```
   Internal Port: 7860 â†’ Gradio UI
   Internal Port: 8000 â†’ vLLM API
   ```
4. ìƒì„±ëœ URLë¡œ ì ‘ì†:
   ```
   https://xxxxx-7860.proxy.runpod.net  (Gradio UI)
   https://xxxxx-8000.proxy.runpod.net  (vLLM API)
   ```

---

## ğŸ”¥ ë¬¸ì œ í•´ê²°

### Dockerê°€ ì—†ëŠ” ê²½ìš°

RunPod PyTorch í…œí”Œë¦¿ì—ëŠ” Dockerê°€ ì—†ìŠµë‹ˆë‹¤!

**í•´ê²° ë°©ë²•:**
```bash
# Python ì§ì ‘ ì‹¤í–‰ ë°©ì‹ ì‚¬ìš©
bash run_python_direct.sh
```

### vLLM ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ

```bash
# ë¡œê·¸ í™•ì¸
docker-compose -f docker-compose.runpod.yml logs vllm-server

# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker-compose -f docker-compose.runpod.yml restart vllm-server
```

### GPU ì¸ì‹ ì•ˆ ë¨

```bash
# NVIDIA Docker Runtime í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# docker-compose.ymlì—ì„œ GPU ì„¤ì • í™•ì¸
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼

HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •:

```bash
# .env íŒŒì¼ì— ì¶”ê°€
HF_HOME=/workspace/.cache/huggingface
```

### í¬íŠ¸ ì¶©ëŒ

```bash
# ì‚¬ìš© ì¤‘ì¸ í¬íŠ¸ í™•ì¸
netstat -tlnp | grep 8000
netstat -tlnp | grep 7860

# ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
docker-compose -f docker-compose.runpod.yml down
```

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

### GPU ë©”ëª¨ë¦¬ (RTX 5090 ê¸°ì¤€)

- **vLLM ëª¨ë¸ ë¡œë”©**: ì•½ 7-8GB VRAM
- **ì¶”ë¡  ì‹œ**: ì¶”ê°€ 2-3GB
- **ì´ ê¶Œì¥**: 16GB+ VRAM

### ë””ìŠ¤í¬ ê³µê°„

- **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**: ì•½ 15GB
- **Docker ì´ë¯¸ì§€**: ì•½ 10GB
- **ì´ ê¶Œì¥**: 50GB+

---

## ğŸ“ ë©˜í† ë‹˜ í”¼ë“œë°± ì™„ì „ ë°˜ì˜

### Before (ê¸°ì¡´ ë°©ì‹)

```bash
# 1. RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# 2. SSH ì ‘ì†
ssh root@xxx.xxx.xxx.xxx

# 3. vLLM ì„¤ì¹˜ (5-10ë¶„)
pip install vllm

# 4. ì„œë²„ ì‹¤í–‰ (ìˆ˜ë™)
python vllm_server.py  # í„°ë¯¸ë„ 1

# 5. UI ì‹¤í–‰ (ìˆ˜ë™)
python app.py  # í„°ë¯¸ë„ 2
```

### After (Docker ë°©ì‹)

```bash
# 1. RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Docker ì§€ì› í…œí”Œë¦¿)
# 2. ì›í´ë¦­ ì‹¤í–‰
bash runpod_docker_simple.sh

# âœ… ë! vLLMì´ ì´ë¯¸ ì„œë¹™ ì¤‘!
# âœ… Gradio UIê°€ ì£¼ì†Œë¡œ í†µì‹  ì¤‘!
```

### í•µì‹¬ ê°œì„  ì‚¬í•­

1. **vLLM ì„¤ì¹˜ ë¶ˆí•„ìš”**: Docker ì´ë¯¸ì§€ì— í¬í•¨
2. **ìˆ˜ë™ ì‹¤í–‰ ë¶ˆí•„ìš”**: docker-composeê°€ ìë™ ì‹¤í–‰
3. **ì—…ë°ì´íŠ¸ ê°„ë‹¨**: `docker pull` í•œ ë²ˆìœ¼ë¡œ ìµœì‹  ë²„ì „
4. **ê´€ë¦¬ í¸í•¨**: `docker-compose ps/logs/restart`ë¡œ ëª¨ë“  ì œì–´
5. **ì£¼ì†Œë¡œ í†µì‹ **: `http://vllm-server:8000/v1` ë¡œ API í˜¸ì¶œ

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### vLLM Docker ê³µì‹ ë¬¸ì„œ

- https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html
- DockerHub: https://hub.docker.com/r/vllm/vllm-openai

### Docker Compose ë¬¸ì„œ

- https://docs.docker.com/compose/

### RunPod ë¬¸ì„œ

- https://docs.runpod.io/
- GPU Pod: https://docs.runpod.io/pods/overview

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤í–‰ ì „ í™•ì¸:

- [ ] RunPod Pod ìƒì„± (Docker ì§€ì› í…œí”Œë¦¿)
- [ ] GPU í• ë‹¹ë¨ (RTX 4090/5090 ê¶Œì¥)
- [ ] ë””ìŠ¤í¬ 50GB+ í™•ë³´
- [ ] Docker ì‘ë™ í™•ì¸ (`docker ps`)
- [ ] í”„ë¡œì íŠ¸ í´ë¡  ì™„ë£Œ
- [ ] `runpod_docker_simple.sh` ì‹¤í–‰
- [ ] vLLM ì„œë²„ health check í†µê³¼
- [ ] Gradio UI ì ‘ì† í™•ì¸
- [ ] RunPod í¬íŠ¸ ë…¸ì¶œ ì„¤ì •
- [ ] ì™¸ë¶€ì—ì„œ ì ‘ì† í…ŒìŠ¤íŠ¸

---

## ğŸ‰ ê²°ë¡ 

**ë©˜í† ë‹˜ ë§ì”€ëŒ€ë¡œ "ë”¸ê¹" ì™„ì„±!**

```bash
# 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ë”¸ê¹!)
docker pull vllm/vllm-openai:latest

# 2. ì‹¤í–‰ (ë”¸ê¹!)
docker-compose -f docker-compose.runpod.yml up -d

# 3. ì ‘ì† (ë”¸ê¹!)
http://localhost:7860
```

**ì´ì œ vLLM ì„¤ì¹˜ë„, ìˆ˜ë™ ì‹¤í–‰ë„ í•„ìš” ì—†ìŠµë‹ˆë‹¤!** ğŸš€
