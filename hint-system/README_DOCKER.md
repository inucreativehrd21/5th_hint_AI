# ğŸš€ vLLM Docker ê¸°ë°˜ íŒíŠ¸ ìƒì„± ì‹œìŠ¤í…œ

ë°±ì¤€ ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ AI íŒíŠ¸ ìƒì„± ì‹œìŠ¤í…œì…ë‹ˆë‹¤. vLLMì˜ ê³µì‹ Docker ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì¶”ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” íŠ¹ì§•

- ğŸ³ **Docker ê¸°ë°˜ ë°°í¬**: vLLM ê³µì‹ ì´ë¯¸ì§€ ì‚¬ìš©, ì˜ì¡´ì„± ê´€ë¦¬ ê°„ì†Œí™”
- âš¡ **ê³ ì† ì¶”ë¡ **: vLLMì˜ PagedAttentionê³¼ Continuous Batching
- ğŸ¨ **Gradio UI**: ì§ê´€ì ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤
- ğŸ”§ **ê°„í¸í•œ ì„¤ì •**: í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ êµ¬ì„±
- ğŸŒ **RunPod ìµœì í™”**: í´ë¼ìš°ë“œ GPU í™˜ê²½ ì™„ë²½ ì§€ì›

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ì‚¬ì–‘ (Qwen2.5-Coder-7B ê¸°ì¤€)
- **GPU**: NVIDIA GPU (CUDA 12.1+)
  - RTX 3090/4090 (24GB VRAM) - ê¶Œì¥
  - RTX A5000 (24GB VRAM)
  - A100 40GB/80GB (ì˜¤ë²„ìŠ¤í™)
- **ë©”ëª¨ë¦¬**: 32GB RAM
- **ë””ìŠ¤í¬**: 50GB (ëª¨ë¸ + ì‹œìŠ¤í…œ)
- **OS**: Linux (Ubuntu 22.04 ê¶Œì¥)

### ì†Œí”„íŠ¸ì›¨ì–´
- Docker 20.10+
- Docker Compose 2.0+
- NVIDIA Container Toolkit

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì €ì¥ì†Œ í´ë¡ 
```bash
git clone https://github.com/<your-username>/5th_project_mvp.git
cd 5th_project_mvp/hint-system
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# í•„ìš”ì‹œ .env í¸ì§‘
nano .env
```

### 3. ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x quick_start.sh

# ë°°í¬ ì‹œì‘
./quick_start.sh
```

### 4. ì ‘ì†
- **Gradio UI**: http://localhost:7860
- **vLLM API**: http://localhost:8000/v1

## ğŸ³ ìˆ˜ë™ ë°°í¬ (ê³ ê¸‰)

### Docker Composeë¡œ ì‹œì‘
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f
```

### ê°œë³„ ì„œë¹„ìŠ¤ ê´€ë¦¬
```bash
# vLLM ì„œë²„ë§Œ ì‹œì‘
docker-compose up -d vllm-server

# Gradio ì•±ë§Œ ì‹œì‘
docker-compose up -d hint-app

# íŠ¹ì • ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart vllm-server

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸ í™•ì¸
docker-compose logs -f vllm-server
```

### ì‹œìŠ¤í…œ ì¤‘ì§€
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose down

# ë³¼ë¥¨ê¹Œì§€ ì œê±° (ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”)
docker-compose down -v
```

## ğŸ”§ ì„¤ì • ê°€ì´ë“œ

### ì£¼ìš” í™˜ê²½ ë³€ìˆ˜ (.env)

```bash
# vLLM ëª¨ë¸ ì„¤ì •
VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct
VLLM_GPU_MEMORY_UTILIZATION=0.85  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (0.0~1.0)
VLLM_MAX_MODEL_LEN=4096            # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´

# Gradio ì•± ì„¤ì •
VLLM_SERVER_URL=http://vllm-server:8000/v1
GRADIO_PORT=7860

# ë°ì´í„° ê²½ë¡œ
DATA_FILE_PATH=data/problems_multi_solution.json
```

### GPU ë©”ëª¨ë¦¬ ìµœì í™”

OOM (Out of Memory) ì—ëŸ¬ ë°œìƒ ì‹œ:
```bash
# .env íŒŒì¼ ìˆ˜ì •
VLLM_GPU_MEMORY_UTILIZATION=0.75  # 0.85 â†’ 0.75ë¡œ ë‚®ì¶¤
VLLM_MAX_MODEL_LEN=2048            # 4096 â†’ 2048ë¡œ ë‚®ì¶¤

# ì¬ì‹œì‘
docker-compose restart vllm-server
```

## ğŸ“Š RunPod ë°°í¬

ìƒì„¸í•œ RunPod ë°°í¬ ê°€ì´ë“œëŠ” [RUNPOD_DEPLOYMENT_GUIDE.md](RUNPOD_DEPLOYMENT_GUIDE.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### ì¶”ì²œ GPU (ê°€ì„±ë¹„)
1. **RTX 4090** (24GB) - ~$0.44/ì‹œê°„ â­â­â­â­â­
2. **RTX 3090** (24GB) - ~$0.39/ì‹œê°„ â­â­â­â­â­
3. **RTX A5000** (24GB) - ~$0.49/ì‹œê°„ â­â­â­â­

### ë¹ ë¥¸ ë°°í¬
```bash
# RunPod ì¸ìŠ¤í„´ìŠ¤ SSH ì ‘ì† í›„
cd /workspace
git clone <repository-url>
cd 5th_project_mvp/hint-system

# ìë™ ë°°í¬
chmod +x quick_start.sh
./quick_start.sh

# í¬íŠ¸ 7860ì„ RunPodì—ì„œ ë…¸ì¶œ
# Console â†’ Connect â†’ HTTP Service â†’ 7860
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì‹œìŠ¤í…œ ê²€ì¦
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦
chmod +x verify_system.sh
./verify_system.sh
```

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```bash
# ë¡œê·¸ í™•ì¸
docker-compose logs vllm-server

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health

# ì¬ì‹œì‘
docker-compose restart vllm-server
```

#### 2. Gradio ì•± ì ‘ì† ë¶ˆê°€
```bash
# ë¡œê·¸ í™•ì¸
docker-compose logs hint-app

# í¬íŠ¸ í™•ì¸
docker-compose port hint-app 7860

# ì¬ì‹œì‘
docker-compose restart hint-app
```

#### 3. ë°ì´í„° íŒŒì¼ ì—†ìŒ
```bash
# ë³¼ë¥¨ ë§ˆìš´íŠ¸ í™•ì¸
docker-compose exec hint-app ls -la /app/data/

# íŒŒì¼ ì¡´ì¬ í™•ì¸
ls -la data/problems_multi_solution.json
```

#### 4. GPU ì¸ì‹ ì•ˆë¨
```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# Dockerì—ì„œ GPU í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# NVIDIA Container Toolkit ì¬ì„¤ì¹˜
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

## ğŸ“ ê°œë°œ ê°€ì´ë“œ

### ë¡œì»¬ ê°œë°œ (vLLM ì„œë²„ë§Œ Dockerë¡œ ì‹¤í–‰)

```bash
# vLLM ì„œë²„ë§Œ ì‹œì‘
docker-compose up -d vllm-server

# ë¡œì»¬ì—ì„œ Gradio ì•± ì‹¤í–‰
pip install -r requirements-app.txt
python app.py --server-name 127.0.0.1 --vllm-url http://localhost:8000/v1
```

### ì½”ë“œ ìˆ˜ì • í›„ ì¬ë¹Œë“œ
```bash
# Gradio ì•±ë§Œ ì¬ë¹Œë“œ
docker-compose build hint-app
docker-compose up -d hint-app

# ì „ì²´ ì¬ë¹Œë“œ
docker-compose down
docker-compose up --build -d
```

## ğŸ“š í”„ë¡œì íŠ¸ êµ¬ì¡°

```
hint-system/
â”œâ”€â”€ app.py                      # Gradio ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”ì¸
â”œâ”€â”€ config.py                   # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ docker-compose.yml          # Docker Compose ì„¤ì •
â”œâ”€â”€ Dockerfile                  # Gradio ì•± Dockerfile
â”œâ”€â”€ requirements-app.txt        # Gradio ì•± ì˜ì¡´ì„±
â”œâ”€â”€ .env.example               # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì‹œ
â”œâ”€â”€ quick_start.sh             # ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ verify_system.sh           # ì‹œìŠ¤í…œ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ RUNPOD_DEPLOYMENT_GUIDE.md # RunPod ë°°í¬ ê°€ì´ë“œ
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_inference.py     # VLLMInference í´ë˜ìŠ¤
â”‚   â””â”€â”€ model_config.py        # ëª¨ë¸ ì„¤ì •
â””â”€â”€ data/
    â””â”€â”€ problems_multi_solution.json  # ë¬¸ì œ ë°ì´í„°
```

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆì™€ í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ™ ê°ì‚¬ì˜ ë§

- [vLLM](https://github.com/vllm-project/vllm) - ê³ ì† LLM ì¶”ë¡  ì—”ì§„
- [Gradio](https://www.gradio.app/) - ì›¹ UI í”„ë ˆì„ì›Œí¬
- [Qwen](https://github.com/QwenLM/Qwen) - ì½”ë“œ ìƒì„± ëª¨ë¸

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì´ìŠˆë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [Docker Compose ë¬¸ì„œ](https://docs.docker.com/compose/)
- [RunPod ë¬¸ì„œ](https://docs.runpod.io/)
