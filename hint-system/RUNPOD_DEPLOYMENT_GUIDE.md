# ============================================================================
# RunPod ë°°í¬ ê°€ì´ë“œ - vLLM Docker ê¸°ë°˜ íŒíŠ¸ ìƒì„± ì‹œìŠ¤í…œ
# ============================================================================

## ğŸ“‹ ëª©ì°¨
1. [RunPod ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ê¶Œì¥ì‚¬í•­](#runpod-ì¸ìŠ¤í„´ìŠ¤-ì„¤ì •-ê¶Œì¥ì‚¬í•­)
2. [ë°°í¬ ë‹¨ê³„](#ë°°í¬-ë‹¨ê³„)
3. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ğŸš€ RunPod ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ê¶Œì¥ì‚¬í•­

### GPU ì„ íƒ (ëª¨ë¸ë³„)

#### **Qwen2.5-Coder-7B-Instruct (ê¶Œì¥)**
| GPU íƒ€ì… | VRAM | ê°€ê²©/ì‹œê°„ | ì¶”ì²œë„ | ë¹„ê³  |
|---------|------|----------|-------|------|
| **RTX 4090** | 24GB | ~$0.44 | â­â­â­â­â­ | **ìµœê³  ê°€ì„±ë¹„** |
| **RTX A5000** | 24GB | ~$0.49 | â­â­â­â­ | ì•ˆì •ì  |
| **RTX 3090** | 24GB | ~$0.39 | â­â­â­â­â­ | ê°€ì„±ë¹„ ìš°ìˆ˜ |
| **RTX A4000** | 16GB | ~$0.29 | â­â­â­ | ìµœì†Œ ì‚¬ì–‘ (tight) |
| **A100 40GB** | 40GB | ~$1.09 | â­â­ | ì˜¤ë²„ìŠ¤í™, ë¹„ìŒˆ |
| **A100 80GB** | 80GB | ~$1.89 | â­ | ë§¤ìš° ë¹„ìŒˆ |

#### **ë” ì‘ì€ ëª¨ë¸ (1.5B ~ 3B)**
- RTX 3060 (12GB): ~$0.19/ì‹œê°„
- RTX 3070 (8GB): ~$0.24/ì‹œê°„

### ë””ìŠ¤í¬ ìš©ëŸ‰
- **ìµœì†Œ**: 40GB (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + ì‹œìŠ¤í…œ)
- **ê¶Œì¥**: 50GB (ì—¬ìœ  ê³µê°„ í™•ë³´)
- **ì£¼ì˜**: Container DiskëŠ” ëª¨ë¸ ìºì‹œê°€ ìœ ì§€ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, Volume Diskë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ HuggingFace ìºì‹œ ê²½ë¡œë¥¼ ì˜êµ¬ ë³¼ë¥¨ì— ë§ˆìš´íŠ¸

### í…œí”Œë¦¿ ì„ íƒ
1. **RunPod PyTorch 2.1** (ê¶Œì¥)
   - ì´ë¯¸ì§€: `runpod/pytorch:2.1.0-py3.10-cuda12.1.0-devel-ubuntu22.04`
   - Dockerì™€ NVIDIA Container Toolkit í¬í•¨
   - GPU ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ

2. **RunPod Docker** (ì§ì ‘ ì„¤ì •)
   - ì´ë¯¸ì§€: `runpod/base:0.4.0-cuda12.1.0`
   - Docker Compose ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”

---

## ğŸ› ï¸ ë°°í¬ ë‹¨ê³„

### 1. RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
1. [RunPod Console](https://www.runpod.io/console/pods)ì—ì„œ **Deploy** í´ë¦­
2. GPU íƒ€ì… ì„ íƒ (ìœ„ í‘œ ì°¸ê³ )
3. í…œí”Œë¦¿: `RunPod PyTorch 2.1` ì„ íƒ
4. Container Disk: 50GB
5. Volume Disk (ì„ íƒ): 100GB (ëª¨ë¸ ìºì‹œ ì˜êµ¬ ì €ì¥)
6. **Expose HTTP Ports**: `7860, 8000` ì¶”ê°€
7. **Deploy** í´ë¦­

### 2. ì¸ìŠ¤í„´ìŠ¤ ì ‘ì†
```bash
# SSH ì ‘ì† (RunPod Consoleì—ì„œ SSH ëª…ë ¹ì–´ ë³µì‚¬)
ssh root@<pod-id>.ssh.runpod.io -p <port> -i ~/.ssh/id_rsa
```

### 3. í”„ë¡œì íŠ¸ í´ë¡ 
```bash
# ì‘ì—… ë””ë ‰í† ë¦¬ ì´ë™
cd /workspace

# Git ì„¤ì¹˜ (í•„ìš”ì‹œ)
apt-get update && apt-get install -y git

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/<your-username>/5th_project_mvp.git
cd 5th_project_mvp/hint-system
```

### 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# .env íŒŒì¼ í¸ì§‘ (í•„ìš”ì‹œ)
nano .env

# í•„ìˆ˜ ì„¤ì • í™•ì¸
# - VLLM_MODEL (ê¸°ë³¸ê°’: Qwen/Qwen2.5-Coder-7B-Instruct)
# - VLLM_GPU_MEMORY_UTILIZATION (ê¸°ë³¸ê°’: 0.85)
# - VLLM_MAX_MODEL_LEN (ê¸°ë³¸ê°’: 4096)
```

### 5. Docker Compose ì„¤ì¹˜ (RunPod Docker í…œí”Œë¦¿ ì‚¬ìš© ì‹œ)
```bash
# Docker Compose ì„¤ì¹˜
apt-get update
apt-get install -y docker-compose-plugin

# ë˜ëŠ”
curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose
```

### 6. vLLM Docker ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ì„ íƒ)
```bash
# vLLM ì´ë¯¸ì§€ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (10~15ë¶„ ì†Œìš”)
docker pull vllm/vllm-openai:latest

# ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ì„ íƒ)
# ì´ë ‡ê²Œ í•˜ë©´ ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ ì ˆì•½
mkdir -p ~/.cache/huggingface
export VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct

# Pythonìœ¼ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python3 -c "from huggingface_hub import snapshot_download; snapshot_download('${VLLM_MODEL}')"
```

### 7. ì‹œìŠ¤í…œ ì‹œì‘
```bash
# Docker Composeë¡œ ì‹œìŠ¤í…œ ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# vLLM ì„œë²„ ë¡œê·¸ë§Œ í™•ì¸
docker-compose logs -f vllm-server

# Gradio ì•± ë¡œê·¸ë§Œ í™•ì¸
docker-compose logs -f hint-app
```

### 8. ì ‘ì† í™•ì¸
```bash
# vLLM ì„œë²„ í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health

# ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:8000/v1/models

# Gradio ì•± í™•ì¸
curl http://localhost:7860/
```

### 9. ì›¹ ë¸Œë¼ìš°ì € ì ‘ì†
RunPod Consoleì—ì„œ **Connect** â†’ **HTTP Service** â†’ í¬íŠ¸ `7860` í´ë¦­
- URL í˜•ì‹: `https://<pod-id>-7860.proxy.runpod.net`

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

### GPU ì‚¬ìš©ë¥  í™•ì¸
```bash
# nvidia-smië¡œ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# Docker ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
docker stats
```

### ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (Qwen2.5-Coder-7B)
- **ëª¨ë¸ ê°€ì¤‘ì¹˜**: ~14GB (FP16)
- **KV Cache**: ~6GB (batch_size=256, seq_len=4096)
- **ê¸°íƒ€**: ~2GB
- **ì´í•©**: ~22GB (RTX 4090 24GBë¡œ ì¶©ë¶„)

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. OOM (Out of Memory) ì—ëŸ¬
```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë‚®ì¶”ê¸°
nano .env
# VLLM_GPU_MEMORY_UTILIZATION=0.85 â†’ 0.75

# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
# VLLM_MAX_MODEL_LEN=4096 â†’ 2048

# ì¬ì‹œì‘
docker-compose restart vllm-server
```

### 2. vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```bash
# vLLM ì„œë²„ ìƒíƒœ í™•ì¸
docker-compose ps

# ë¡œê·¸ í™•ì¸
docker-compose logs vllm-server

# ì¬ì‹œì‘
docker-compose restart vllm-server

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health
```

### 3. Gradio ì•± ì ‘ì† ì•ˆë¨
```bash
# ì•± ìƒíƒœ í™•ì¸
docker-compose ps hint-app

# ë¡œê·¸ í™•ì¸
docker-compose logs hint-app

# í¬íŠ¸ í™•ì¸ (7860 ë…¸ì¶œë˜ì—ˆëŠ”ì§€)
docker-compose port hint-app 7860

# ì¬ì‹œì‘
docker-compose restart hint-app
```

### 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼
```bash
# HuggingFace í† í° ì„¤ì • (ì„ íƒ)
nano .env
# HUGGING_FACE_HUB_TOKEN=your_token_here

# ì¬ì‹œì‘
docker-compose down
docker-compose up -d
```

### 5. ì „ì²´ ì‹œìŠ¤í…œ ì¬ì‹œì‘
```bash
# ëª¨ë“  ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ë° ì œê±°
docker-compose down

# ë³¼ë¥¨ê¹Œì§€ ì œê±° (ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”)
docker-compose down -v

# ì¬ì‹œì‘
docker-compose up -d
```

---

## ğŸ¯ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. GPU ë©”ëª¨ë¦¬ ìµœì í™”
- **Prefix Caching í™œì„±í™”**: `--enable-prefix-caching` (ì´ë¯¸ ì ìš©ë¨)
- **Flash Attention ì‚¬ìš©**: `--use-flash-attention` (Ampere ì´ìƒ GPU)

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- `--max-num-seqs`: ë™ì‹œ ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìˆ˜ (ê¸°ë³¸ 256)
- `--max-num-batched-tokens`: ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜

### 3. ë‹¤ì¤‘ GPU ì‚¬ìš©
```bash
# .env íŒŒì¼ ìˆ˜ì •
VLLM_TENSOR_PARALLEL_SIZE=2  # 2ê°œ GPU ì‚¬ìš©

# ì¬ì‹œì‘
docker-compose restart vllm-server
```

---

## ğŸ“ ì°¸ê³  ìë£Œ
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [vLLM Docker Hub](https://hub.docker.com/r/vllm/vllm-openai)
- [RunPod ë¬¸ì„œ](https://docs.runpod.io/)
- [Qwen2.5-Coder ëª¨ë¸](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)

---

## ğŸ’¡ ì¶”ê°€ ì •ë³´

### ë¹„ìš© ì ˆê° íŒ
1. **Spot Instances ì‚¬ìš©**: On-Demand ëŒ€ë¹„ ~50% ì €ë ´
2. **Auto-Stop ì„¤ì •**: ìœ íœ´ ì‹œê°„ í›„ ìë™ ì¤‘ì§€
3. **ì‘ì€ ëª¨ë¸ ì‹œë„**: Qwen2.5-Coder-1.5B-Instruct (RTX 3060 12GBë¡œ ê°€ëŠ¥)

### ë³´ì•ˆ ê¶Œì¥ì‚¬í•­
1. SSH í‚¤ ì¸ì¦ ì‚¬ìš©
2. í•„ìš”í•œ í¬íŠ¸ë§Œ ë…¸ì¶œ
3. í™˜ê²½ ë³€ìˆ˜ì— ë¯¼ê° ì •ë³´ ì €ì¥ (`.env` íŒŒì¼ì„ Gitì— ì»¤ë°‹í•˜ì§€ ì•Šê¸°)
