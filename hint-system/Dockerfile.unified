# vLLM ê³µì‹ ì´ë¯¸ì§€ ë² ì´ìŠ¤
FROM vllm/vllm-openai:latest

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ ë° í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜ (ë‹¨ì¼ ë ˆì´ì–´, ìºì‹œ ì œê±°)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    git \
    vim \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /tmp/* \
    && rm -rf /var/tmp/*

# Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„± íŒŒì¼ ë³µì‚¬
COPY requirements.txt /app/requirements.txt

# Gradio ë° ì•± ì˜ì¡´ì„± ì„¤ì¹˜ (vLLM 0.11.0 í˜¸í™˜ ë²„ì „)
RUN pip install --no-cache-dir \
    gradio==4.44.0 \
    transformers>=4.55.2 \
    tokenizers>=0.21.1 \
    accelerate>=0.27.0 \
    sentencepiece \
    protobuf \
    openai>=1.3.0 \
    requests \
    python-dotenv \
    && pip install --no-cache-dir -r requirements.txt || true \
    && python3 -c "import transformers; print('âœ… TRANSFORMERS_VERSION=' + transformers.__version__)" \
    && rm -rf /root/.cache/pip \
    && rm -rf /tmp/*

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . /app

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV VLLM_API_BASE=http://127.0.0.1:8000/v1
ENV VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct
ENV HF_HOME=/root/.cache/huggingface
ENV VLLM_SERVER_URL=http://127.0.0.1:8000/v1
ENV GRADIO_SERVER_NAME=0.0.0.0
ENV GRADIO_SERVER_PORT=7860

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000 7860

# ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
RUN echo '#!/bin/bash\n\
set -e\n\
\n\
echo "ğŸš€ Starting vLLM + Gradio Hint System..."\n\
echo "========================================"\n\
echo "ğŸ“¦ Model: Qwen/Qwen2.5-Coder-7B-Instruct"\n\
echo "ğŸ’¾ GPU Memory: 90%"\n\
echo "========================================"\n\
\n\
# vLLM ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)\n\
echo "ğŸ“¦ Starting vLLM server on port 8000..."\n\
python3 -m vllm.entrypoints.openai.api_server \\\n\
  --model Qwen/Qwen2.5-Coder-7B-Instruct \\\n\
  --host 0.0.0.0 \\\n\
  --port 8000 \\\n\
  --dtype auto \\\n\
  --max-model-len 4096 \\\n\
  --gpu-memory-utilization 0.90 \\\n\
  --trust-remote-code &\n\
\n\
VLLM_PID=$!\n\
echo "âœ… vLLM server started (PID: $VLLM_PID)"\n\
\n\
# vLLM ì„œë²„ ì¤€ë¹„ ëŒ€ê¸°\n\
echo "â³ Waiting for vLLM server to be ready..."\n\
for i in {1..60}; do\n\
  if curl -s http://localhost:8000/health > /dev/null 2>&1; then\n\
    echo "âœ… vLLM server is ready!"\n\
    break\n\
  fi\n\
  sleep 5\n\
  if [ $((i % 6)) -eq 0 ]; then\n\
    echo "   Still waiting... ($((i * 5))s)"\n\
  fi\n\
done\n\
\n\
# Gradio UI ì‹œì‘ (í¬ê·¸ë¼ìš´ë“œ)\n\
echo "ğŸ¨ Starting Gradio UI on port 7860..."\n\
cd /app\n\
python3 app.py --server-name 0.0.0.0 --server-port 7860\n\
' > /app/start.sh && chmod +x /app/start.sh

# ì»¨í…Œì´ë„ˆ ì‹œì‘ ëª…ë ¹
CMD ["/app/start.sh"]
