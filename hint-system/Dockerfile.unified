# vLLM ê³µì‹ ì´ë¯¸ì§€ ë² ì´ìŠ¤
FROM vllm/vllm-openai:latest

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ ë° í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜ (ë‹¨ì¼ ë ˆì´ì–´, ìºì‹œ ì œê±°)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    git \
    vim \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /tmp/* \
    && rm -rf /var/tmp/*

# Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„± íŒŒì¼ ë³µì‚¬
COPY requirements.txt /app/requirements.txt

# Gradio ë° ì•± ì˜ì¡´ì„± ì„¤ì¹˜ (vLLM 0.11.0 í˜¸í™˜ ë²„ì „)
RUN pip install --no-cache-dir \
    gradio==4.44.0 \
    transformers>=4.55.2 \
    tokenizers>=0.21.1 \
    accelerate>=0.27.0 \
    sentencepiece \
    protobuf \
    openai>=1.3.0 \
    requests \
    python-dotenv \
    && pip install --no-cache-dir -r requirements.txt || true \
    && python3 -c "import transformers; print('âœ… TRANSFORMERS_VERSION=' + transformers.__version__)" \
    && rm -rf /root/.cache/pip \
    && rm -rf /tmp/*

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . /app

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV VLLM_API_BASE=http://127.0.0.1:8000/v1
ENV VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct
ENV VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct
ENV HF_HOME=/root/.cache/huggingface
ENV VLLM_SERVER_URL=http://127.0.0.1:8000/v1
ENV GRADIO_SERVER_NAME=0.0.0.0
ENV GRADIO_SERVER_PORT=7860

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000 7860

# ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
RUN echo '#!/bin/bash\n\
\n\
echo "=========================================="\n\
echo "ğŸš€ vLLM + Gradio Hint System Starting..."\n\
echo "=========================================="\n\
echo "Time: $(date)"\n\
echo "Model: Qwen/Qwen2.5-Coder-7B-Instruct"\n\
echo "GPU Memory: 90%"\n\
echo "=========================================="\n\
echo ""\n\
\n\
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\
export VLLM_MODEL="Qwen/Qwen2.5-Coder-7B-Instruct"\n\
export VLLM_MODEL_NAME="Qwen/Qwen2.5-Coder-7B-Instruct"\n\
export DEFAULT_MODEL="Qwen/Qwen2.5-Coder-7B-Instruct"\n\
export HF_HOME=/root/.cache/huggingface\n\
\n\
echo "ğŸ“‚ Model cache directory:"\n\
ls -lh /root/.cache/huggingface/hub/ 2>/dev/null || echo "  (not yet cached)"\n\
echo ""\n\
\n\
# vLLM ì„œë²„ ì‹œì‘\n\
echo "ğŸ“¦ Starting vLLM server..."\n\
echo "   Port: 8000"\n\
echo "   Model: Qwen/Qwen2.5-Coder-7B-Instruct"\n\
echo "   Log: /app/vllm.log"\n\
\n\
nohup python3 -m vllm.entrypoints.openai.api_server \\\n\
  --model "Qwen/Qwen2.5-Coder-7B-Instruct" \\\n\
  --served-model-name "Qwen/Qwen2.5-Coder-7B-Instruct" \\\n\
  --host 0.0.0.0 \\\n\
  --port 8000 \\\n\
  --dtype auto \\\n\
  --max-model-len 4096 \\\n\
  --gpu-memory-utilization 0.90 \\\n\
  --trust-remote-code > /app/vllm.log 2>&1 &\n\
\n\
VLLM_PID=$!\n\
echo "âœ… vLLM server started (PID: $VLLM_PID)"\n\
echo ""\n\
\n\
# vLLM í”„ë¡œì„¸ìŠ¤ í™•ì¸\n\
sleep 5\n\
if kill -0 $VLLM_PID 2>/dev/null; then\n\
  echo "âœ… vLLM process is running"\n\
else\n\
  echo "âŒ vLLM failed to start!"\n\
  echo "Last 30 lines of log:"\n\
  tail -30 /app/vllm.log 2>/dev/null || echo "No log file"\n\
  echo ""\n\
  echo "âš ï¸  Continuing anyway (will retry connection)..."\n\
fi\n\
echo ""\n\
\n\
# vLLM ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ 5ë¶„)\n\
echo "â³ Waiting for vLLM server health check..."\n\
WAIT_COUNT=0\n\
MAX_WAIT=60\n\
while [ $WAIT_COUNT -lt $MAX_WAIT ]; do\n\
  if curl -s http://localhost:8000/health > /dev/null 2>&1; then\n\
    echo "âœ… vLLM server is ready!"\n\
    break\n\
  fi\n\
  WAIT_COUNT=$((WAIT_COUNT + 1))\n\
  if [ $((WAIT_COUNT % 6)) -eq 0 ]; then\n\
    echo "   Still waiting... ($((WAIT_COUNT * 5))s / $((MAX_WAIT * 5))s)"\n\
    if ! kill -0 $VLLM_PID 2>/dev/null; then\n\
      echo "   âš ï¸  vLLM process died, check /app/vllm.log"\n\
    fi\n\
  fi\n\
  sleep 5\n\
done\n\
\n\
if [ $WAIT_COUNT -eq $MAX_WAIT ]; then\n\
  echo "âš ï¸  vLLM health check timeout (continuing anyway)"\n\
  echo "Last 20 lines of vLLM log:"\n\
  tail -20 /app/vllm.log 2>/dev/null || echo "No log file"\n\
fi\n\
echo ""\n\
\n\
# Gradio UI ì‹œì‘\n\
echo "ğŸ¨ Starting Gradio UI..."\n\
echo "   Port: 7860"\n\
echo "   Server: 0.0.0.0"\n\
echo "=========================================="\n\
echo ""\n\
\n\
cd /app || exit 1\n\
exec python3 app.py --server-name 0.0.0.0 --server-port 7860\n\
' > /app/start.sh && chmod +x /app/start.sh

# RunPod í˜¸í™˜ ì‹œì‘ ëª…ë ¹ (ê°„ë‹¨í•œ ë°©ì‹)
CMD ["/bin/bash", "-c", "/app/start.sh"]
